#!/usr/bin/perl

# This file is part of runtest-utils.
#
# runtest-utils is free software: you can redistribute it and/or modify
# it under the terms of the GNU General Public License as published by
# the Free Software Foundation, either version 3 of the License, or
# (at your option) any later version.
#
# runtest-utils is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
# GNU General Public License for more details.
#
# You should have received a copy of the GNU General Public License
# along with runtest-utls.  If not, see <https://www.gnu.org/licenses/>.

use warnings;
use strict;
no indirect;
no autovivification;

# Converted to a real package number by autotools.
my $VERSION_NUMBER = "~~PACKAGE-VERSION~~";

#========================================================================#

=pod

=head1 NAME

sum4way - 4 way comparison between 4 summary files

=head1 OPTIONS

B<sum4way> [-h|--help]
           [-o|--org]
           [-d|--dump-ids]
           [-t <TARGET>|--target=<TARGET>]
           NAME1=PATH1 NAME2=PATH2 NAME3=PATH3 NAME4=PATH4

=head1 SYNOPSIS

Imagine a development tree that looks a little like this:

   |
   |
   v

   A ------> B

   |
   |
   |
   |
   v

   C ------> D

   |
   |
   v

Where I<A> and I<C> are both revisions on the main development branch,
while I<B> and I<D> are patched versions based off of A and C respectively.

Further, the same patch set that created B from A was rebased to create D
from C.

The question then, is did we rebase the patch-set correctly, or did we
introduce some regressions.  To answer this question we run the full set of
available tests on all 4 worlds, A, B, C, and D, this script will then
compare the results.

Some of the issues to consider are this, the patch A to B might cause a
test to fail, we would then expect the test to fail between C and D, this
script detects and accepts these regressions.

A new test might be added as part of the patch A to B, this new test might
pass, or might fail, this script will expect either the same, or a better
result between C and D.

New tests might be added between A and C, these tests might pass or fail,
this script will expect the same new tests to appear in D and their results
should be no worse than they are in C.

In order to identify each component in the above tree then, when specifying
the path to each gdb summary file a name should also be supplied, this name
is used when printing the results.

Finally in the above tree NAME1 is A, NAME2 is B, NAME3 is C, and NAME4 is
D.

The optional I<-o> or I<--org> option changes the output format, instead of
a plain text list, the output is produced in emacs org-mode format.

The I<-d> or I<--dump-ids> is for debug only, this dumps a list of all test
IDs, and can be used to check which IDs the script is finding.

When processing summary files with multiple targets in then the user must
select which targets to compare using the I<--target> option.  The value
passed to this option is a string, the name of a target.  If the script is
not passed this option and the summary files have multiple targets, then a
list of the available targets will be printed.

=cut

#========================================================================#

use FindBin;
use lib "$FindBin::Bin/lib";
use GiveHelp qw/usage/;         # Allow -h or --help command line options.
use boolean;
use Carp;
use Carp::Assert;
use RunTestUtils::SumFile;
use RunTestUtils::FilterManager;
use Getopt::Long;

my $SYSCONFDIR = "$FindBin::Bin/etc";
my $FILTERS_PATH = "$SYSCONFDIR/runtest-utils/filters/";

#========================================================================#

my $org_format = false;
my $dump_ids = false;
my $show_version = false;
my $target_name = undef;

GetOptions ("version" => \$show_version,
            "org|o" => \$org_format,
            "dump-ids|d" => \$dump_ids,
            "target|t=s" => \$target_name);

if ($show_version)
{
  print "$FindBin::Script version $VERSION_NUMBER\n";
  exit (0);
}

RunTestUtils::FilterManager::load_filters ($FILTERS_PATH);

my ($rev_a, $rev_b, $rev_c, $rev_d) = process_argv (@ARGV);

if ($dump_ids)
{
  dump_all_ids ($rev_a, $rev_b, $rev_c, $rev_d);
  exit (0);
}

my @fail_a;
foreach my $id (keys %{$rev_b->{-results}})
{
  # If this is a test that passes on 'B', and...
  my $r = $rev_b->{-results}->{ $id };
  next if ($r->is_bad ());

  # ... the test either didn't exist, or used to fail on 'A', then...
  my $old = $rev_a->{-results}->{ $id };
  next if (defined ($old) and (not ($old->is_bad ())));
  my $os = "GONE";
  $os = $old->get_status () if (defined ($old));

  # ... if the test is not passing on 'D', then...
  my $new = $rev_d->{-results}->{ $id };
  next if (defined ($new) and (not ($new->is_bad ())));
  my $ns = "GONE";
  $ns = $new->get_status () if (defined ($new));

  # ... we are interested in this test.
  push @fail_a, { -result => $r,
                  -string => $os ." -> ". $r->get_status () ." => ". $ns }
}

my @fail_b;
foreach my $id (keys %{$rev_c->{-results}})
{
  # If this is a test that passes on 'C', and...
  my $r = $rev_c->{-results}->{ $id };
  next unless ($r->is_pass ());

  # ... and it has either gone away, or started to fail in 'D', then...
  my $new = $rev_d->{-results}->{ $id };
  next if (defined ($new) and not ($new->is_bad ()));
  my $ts = ((defined ($new)) ? $new->get_status () : "GONE");

  # ... we are interested in this test, unless, the test already existed on
  # 'A' and was failing on 'B', which would indicate this is a known
  # regression.
  if ((exists ($rev_a->{-results}->{ $id }))
         and ($rev_a->{-results}->{ $id }->get_status ()
              eq $r->get_status ()))
  {
    my $old = $rev_b->{-results}->{ $id };
    if ((not (defined ($new)))
          and (not (defined ($old))))
    {
      # This test was removed from 'B' and is also removed from 'D', we
      # accept this transition to the GONE state.
      next;
    }

    if ((defined ($new)) and (defined ($old))
          and ($new->get_status () eq $old->get_status ()))
    {
      # This test ended in the same state in 'D' as it did in 'B', we
      # assume that we're happy with the state in 'B', so we should also
      # accept the state in 'D'.
      next;
    }
  }

  push @fail_b, { -result => $r,
                  -string => $r->get_status () ." -> ". $ts };
}

my @fail_c;
foreach my $id (keys %{$rev_d->{-results}})
{
  # If this is a test that fails on 'D', and...
  my $r = $rev_d->{-results}->{ $id };
  next unless ($r->is_bad ());

  # ...if this is a test that didn't exist on 'C', and...
  $r = $rev_c->{-results}->{ $id };
  next unless (not (defined ($r)));

  # ... the test either didn't exist on 'B'...
  $r = $rev_b->{-results}->{ $id };
  next unless (not (defined ($r)));

  $r = $rev_d->{-results}->{ $id };
  push @fail_c, { -result => $r,
                  -string => "GONE -> " . $r->get_status () };
}

##########################################################################
# Try to identify new UNSUPPORTED tests that have appeared in 'D'.  The
# idea is that a whole test file will be marked as unsupported, so one
# UNSUPPORTED test will account for many tests seeming to disappear.
##########################################################################

my %unsupported;
foreach my $id (keys %{$rev_d->{-results}})
{
  my $r = $rev_d->{-results}->{ $id };
  next unless ($r->get_status () eq "UNSUPPORTED");

  my $test_path = $r->get_path ();
  $unsupported { $test_path } = $r;
}

foreach my $id (keys %{$rev_b->{-results}})
{
  my $r = $rev_b->{-results}->{ $id };
  next unless ($r->get_status () eq "UNSUPPORTED");

  my $test_path = $r->get_path ();
  delete $unsupported{ $test_path };
}

##########################################################################
# Now sort all of the test results.
##########################################################################

@fail_a = sort {
  $a->{ -result }->get_id () cmp $b->{ -result }->get_id ()
} @fail_a;

@fail_b = sort {
  $a->{ -result }->get_id () cmp $b->{ -result }->get_id ()
} @fail_b;

@fail_c = sort {
  $a->{ -result }->get_id () cmp $b->{ -result }->get_id ()
} @fail_c;

if ($org_format)
{
  print "* Regressions between ".$rev_b->{-name}." and ".$rev_d->{-name}." [/]\n";
  print ":PROPERTIES:\n";
  print ":COOKIE_DATA: recursive\n";
  print ":END:\n";
  print "  The following tests were introduced, or started working\n";
  print "  between ".$rev_a->{-name}." and ".$rev_b->{-name}.
    ", but are failing on ".$rev_d->{-name}.":\n";
}
else
{

  print "The following tests were introduced, or started working\n";
  print "    between ".$rev_a->{-name}." and ".$rev_b->{-name}.
    ", but are failing on\n";
  print "    ".$rev_d->{-name}.":\n";
  print "    Count = ".scalar (@fail_a)."\n\n";
}

my $prev_test_path = "";
foreach my $t (@fail_a)
{
  my $str = $t->{ -string };
  my $t = $t->{ -result };
  my $test_path = $t->get_path ();
  if ($org_format)
  {
    if ($prev_test_path ne $test_path)
    {
      print "  + ".$test_path." [/]\n";
      $prev_test_path = $test_path;
    }
    print "    + [ ] ".$str. " :: ".$t->get_id ()."\n";
  }
  else
  {
    print "   ".$str. " :: ".$t->get_id ()."\n";
  }
}

if ($org_format)
{
  print "\n";
  print "* Regressions between ".$rev_c->{-name}." and ".$rev_d->{-name}." [/]\n";
  print ":PROPERTIES:\n";
  print ":COOKIE_DATA: recursive\n";
  print ":END:\n";
  print "  The following tests passed on ".$rev_c->{-name}.", but are failing on\n";
  print "  ".$rev_d->{-name}.":\n";
}
else
{
  print "\n\n\n";
  print "The following tests passed on ".$rev_c->{-name}.", but are failing on\n";
  print "    ".$rev_d->{-name}.":\n";
  print "    Count = ".scalar (@fail_b)."\n\n";
}

$prev_test_path = "";
foreach my $t (@fail_b)
{
  my $str = $t->{ -string };
  $t = $t->{ -result };
  my $test_path = $t->get_path ();
  next if (exists $unsupported{ $test_path });
  if ($org_format)
  {
    if ($prev_test_path ne $test_path)
    {
      print "  + ".$test_path." [/]\n";
      $prev_test_path = $test_path;
    }

    print "    + [ ] ".$str." :: ".$t->get_id ()."\n";
  }
  else
  {
    print "   ".$str." :: ".$t->get_id ()."\n";
  }
}

if ($org_format)
{
  print "\n";
  print "* New failures in ".$rev_d->{-name}." [/]\n";
  print ":PROPERTIES:\n";
  print ":COOKIE_DATA: recursive\n";
  print ":END:\n";
  print "  The following tests are new in ".$rev_d->{-name}.", but are failing:\n";
}
else
{
  print "\n\n\n";
  print "The following tests are new in ".$rev_d->{-name}.", but are failing:\n";
  print "    Count = ".scalar (@fail_c)."\n\n";
}

$prev_test_path = "";
foreach my $t (@fail_c)
{
  my $str = $t->{ -string };
  $t = $t->{ -result };
  my $test_path = $t->get_path ();
  next if (exists $unsupported{ $test_path });
  if ($org_format)
  {
    if ($prev_test_path ne $test_path)
    {
      print "  + ".$test_path." [/]\n";
      $prev_test_path = $test_path;
    }

    print "    + [ ] ".$str." :: ".$t->get_id ()."\n";
  }
  else
  {
    print "   ".$str." :: ".$t->get_id ()."\n";
  }
}

#========================================================================#

=pod

=head1 METHODS

The following methods are defined in this script.

=over 4

=cut

#========================================================================#

=pod

=item B<dump_all_ids>

Dump all IDs from all data sets passed as arguments.  Each ID is prefixed
with a character 'A', 'B', 'C', or 'D'.

=cut

sub dump_all_ids {
  my $char = 'A';
  while (my $rev = shift (@_))
  {
    foreach my $id (keys %{$rev->{-results}})
    {
      print "$char: $id\n";
    }

    $char++;
    ($char eq 'Z') and
      die "too many data sets";
  }
}

#========================================================================#

=pod

=item B<process_argv>

Currently undocumented.

=cut

sub process_argv {
  my @names_and_paths = @_;

  my @r;
  foreach my $np (@names_and_paths)
  {
    my ($name, $path) = split /=/, $np;

    if (defined ($name)
          and not (defined ($path))
          and (-f $name)
          and (-r $name))
    {
      $path = $name;
      $name = undef;
    }

    (defined $name) or
      die "Missing NAME in '$np'\n";
    (defined $path) or
      die "Missing PATH in '$np'\n";

    push @r, { -name => $name,
               -path => $path,
               -results => load_sum_file ($path) };
  }

  (scalar (@r) == 4) or
    die "Expected 4 NAME=PATH pairs on the command line\n";
  return @r;
}

#========================================================================#

=pod

=item B<load_sum_file>

Take the filename of a summary file, load the results, and return a
reference to a hash of the results, the keys of the has are the result ID
strings, and the hash values are the TestResult object references.

=cut

sub load_sum_file {
  my $filename = shift;

  my $summary_file = RunTestUtils::SumFile->parse ($filename);
  my @targets = $summary_file->targets ();

  my $selected_target = undef;
  if (scalar (@targets) == 1)
  {
    if ((not defined $target_name) or
          ($target_name eq $targets[0]))
    {
      $selected_target = $targets[0];
    }
  }
  elsif (defined $target_name)
  {
    foreach my $t (@targets)
    {
      if ($t eq $target_name)
      {
        $selected_target = $t;
        last;
      }
    }
  }

  if (not defined $selected_target)
  {
    print "File '$filename' contains the following targets:\n";
    print "  ".join (", ", @targets)."\n";
    print "Please select one of these using --target=TARGET.\n";
    die "unable to select suitable target";
  }

  my @results = $summary_file->results ($selected_target);

  my %res;
  foreach my $r (@results)
  {
    if (exists ($res {$r->get_id ()}))
    {
      my $old = $res {$r->get_id ()};
      ($old->get_status () eq $r->get_status ())
        or warn ("Duplicate test with difference status: '".$r->get_id ().
                 "' in: ".$filename."\n");
    }
    $res {$r->get_id ()} = $r;
  }

  return \%res;
}

#========================================================================#

=pod

=back

=cut
